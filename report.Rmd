---
title: 'A Bayesian approach to analyzing urinary biomarkers for pancreatic cancer'
author: "Noora Torpo, Joel Himanen"
date: "6.12.2021"
output:
  rmarkdown::pdf_document:
    toc: true
    toc_depth: 3
    fig_caption: yes
    includes:  
      in_header: preamble-latex.tex
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Introduction

Pancreatic cancer, or more accurately, its most common form, *Pancreatic ductal adenocarcinoma* (PDAC), is one of the most lethal forms of cancer.
It is often diagnosed at a very late stage, which, combined to its heterogenous base of genetic mutations, leads to an average 5-year survival rate of less than 10%.[1] Therefore, research on early signs of PDAC can be considered to be of utmost importance.

This study focuses on conducting Bayesian inference on a dataset, which holds measurements of potential biomarkers for PDAC.
We aim to model the biomarker levels using prior information to generate posterior distributions using two different approaches in Stan and R.
The goal is to see if and how the distributions for the different biomarkers differ between healthy and positively diagnosed test subjects.

# 2 Data and workflow

The data used in this study dates back to research done by Debernardi et al. in 2020.[2] The set includes information and measurements from 590 test subjects, the measurements considered in this study being amounts of four different proteins measured from urine samples:

-   **Creatinine**, commonly used to indicate kidney function
-   **LYVE1** (lymphatic vessel endothelial hyaluronan receptor 1), may have a part in tumor metastasis
-   **REG1B**, potentially related to pancreas regeneration
-   **TFF1** (trefoil factor 1), may be associated with regeneration and repair of the urinary tract

Of these, the three latter ones are considered target biomarkers, that could potentially help early diagnosis of PDAC.

Test subjects are divided into three different groups:

-   1 = Healthy control subjects (183 samples)
-   2 = Subjects with some beningn pancreatic disease (208 samples)
-   3 = Subjects with a positive PDAC diagnosis (199 samples)

Figure 1 is a visualization of these observations. Each histogram represents one of the four measured protein amounts. The subject groups are color coded (1 = green, 2 = blue, 3 = red), with overlaps presented as mixtures of the overlapping colors.  

```{r, echo=FALSE, out.width="60%", out.height="30%", fig.cap="Histograms of the observed data", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/creatinine_data.png','./images/lyve1_data.png', './images/reg1b_data.png', './images/tff1_data.png'))
``` 

The workflow of this study attempts to fit some probability distributions to each of the four protein levels in all three subject groups.
This is done by choosing weakly to somewhat informative priors for the parameters of these target distributions.
Using Stan, we then compute the likelihoods for our observations, and generate the resulting posterior distributions.
Our hypothesis is, that the posterior distributions of at least some of the biomarkers differ considerably between healthy and PDAC-positive groups.
We will also compare two different modeling approaches:

-   A pooled model, where we assume all three subject groups have the same prior distributions
-   A hierarchical model, where each prior has differently distributed hyperparameters

Data analysis in the original paper was done using "classical" statistical learning theory and tests.
To our knowledge, no other analyses have been performed on this dataset, so we can assume that this is the first time a Bayesian approach has been used on this case.

# 3 Models

In this problem setting, we are fitting two types of models for the data: a pooled model and a hierarchical model. The goal is to find distributions for the proteins in all three diagnosis groups. Therefore, the input data includes three matrices including the data and three scalars indicating the number of observations. Note that the groups have different lengths. The data is denoted by $y_i$, $i=1,2,3$ corresponding to each group, where each column presents four different proteins. Similarly the number of observations is denoted as $n_i$. 

For the pooled model, we are using the same parameters for all of the groups. This means that, in practice, we are not considering the groups but the whole dataset. For the hierarchical model, the distributions have shared hyperparameters but for different groups, the distributions are fitted separately.

As mentioned above, we are considering four proteins (three of which are considered PDAC biomarkers): creatinine, LYVE1, REG1B and TFF1. In both models, the distributions are fitted separately for each protein. After considering the distributions of the data, we decided to use different types of models for creatinine and the three biomarkers.

For creatinine data, an exponential model with two parameters is used. When considering the histogram in figure 1, we can see that this is quite a natural choice. In practice, this means that we are fitting a gamma distribution, as we are using mathematically more general formulation of exponential model, as described in the Bayesian Data Analysis book.[3]

For the three biomarkers LYVE1, REG1B and TFF1, we first tried to use an exponential model, but it turned out that it could not be fitted to the data sensibly. The differences between the largest and the smallest values are drastic, and especially for REG1B and TFF1, the parameters obtain values that are practically zero. Therefore, we decided to make a logarithmic transformation to the data. A visualization of the resulting log-distributions can be seen in figures 3-6 in chapter 7. Note that we are using a natural logarithm.

As some of the values are negative, we decided to simply use a Gaussian model. A skewed distribution might be a better choice, but the most obvious option, such as log-normal distribution, requires values to be positive. To avoid overfitting and unnecessary transformations, we decided to fit a Gaussian model for the three biomarkers. In addition, after logarithmic transformation, the histograms of the three biomarkers were similar enough for us to use the same prior distributions for their parameters, making the implementation more straightforward.

## Pooled model

The exponential model fitted to the creatinine data is formulated below. Note that the choice of priors will be discussed in chapter 4. The first column of the data is denoted as $y_i^1$ corresponding to creatinine data for diagnosis group $i$. The gamma distribution is denoted as $\Gamma$.

$$
\begin{aligned}
y_i^1 &\sim \Gamma(\alpha, \beta) \hspace{3mm} | \hspace{3mm} i=1,2,3 \\
\alpha &\sim \Gamma(1,1) \\
\beta &\sim \Gamma(0.5,1)
\end{aligned}
$$

The Gaussian model fitted to the data for the three biomarkers is formulated below. Indexing $j=2,3,4$ refers to the proteins LYVE1, REB1B and TFF1 respectively.

$$
\begin{aligned}
ln(y_i^j) &\sim \mathcal{N}\left(\mu^j, {\sigma^j}^2\right) \hspace{3mm} | \hspace{3mm} i=1,2,3 \hspace{2mm} \text{and} \hspace{2mm} j=2,3,4 \\
\mu^j &\sim \mathcal{N}\left(0,20^2\right)  \\
\sigma^j &\sim \Gamma(1,1)  \\
\end{aligned}
$$

## Hierarchical model

The hierarchical model is constructed similarly, but the prior distributions use hyperparameters, and the actual parameters are specific for each diagnosis group. In general, we will denote hyperparameters with lower index $P$. The corresponding hierarchical formulation of the exponential model to the creatinine data can be written as follows:


$$
\begin{aligned}
y_i^1 &\sim \Gamma(\alpha_i, \beta_i) \hspace{3mm} | \hspace{3mm} i=1,2,3 \\
\alpha_i &\sim \Gamma(\alpha_{P_1},\beta_{P_1}) \\
\beta_i &\sim \Gamma(\alpha_{P_1},\beta_{P_1}) \\
\\
\alpha_{P_k} &\sim \Gamma(1,1) \hspace{3mm} | \hspace{3mm} k=1,2 \\
\beta_{P_k} &\sim \Gamma(1,1)
\end{aligned}
$$

The corresponding hierarchical formulation of the Gaussian model fitted to the data for three other proteins can be written as follows.

$$
\begin{aligned}
ln(y_i^j) &\sim \mathcal{N}\left(\mu^j_i, {\sigma^j_i}^2\right) \hspace{3mm} | \hspace{3mm} i=1,2,3 \hspace{2mm} \text{and} \hspace{2mm} j=2,3,4 \\
\mu^j_i &\sim \mathcal{N}\left(\mu_{P_1}^j,{\sigma_{P_1}^j}^2\right)   \\
\sigma^j_i &\sim \Gamma\left(\mu_{P_2}^j,\sigma_{P_2}^j\right) \\
\mu_{P_1}^j &\sim \mathcal{N}\left(0,20^2\right) \\
\mu_{P_2}^j &\sim \Gamma(1,1) \\
\sigma_{P_k}^j &\sim \Gamma(1,1) \hspace{3mm} | \hspace{3mm} k=1,2
\end{aligned}
$$

# 4 Priors

In general, the models require two kind of prior distributions. For parameters of the gamma distribution, it is essential to keep them positive, which narrows the possible choice of priors. As presented in the course book Bayesian Data Analysis [3], a gamma distribution is a conjugate prior for an exponential model. Therefore, a gamma distribution is a natural choice for prior distributions for both parameters $\alpha$ and $\beta$ in the case of creatinine. The gamma distribution is naturally quite narrow, so we decided to consider informative priors, but aim for enough flexibility in the model. 

The first parameter of a gamma distribution $\alpha$ determines the shape of the distribution, and the second parameter $\beta$ the scale. After inspecting the creatinine data, we decided to roughly aim for the expected value of 1 for the shape and the expected value of 0.5 for the scale parameter. The expected value of a gamma distribution is calculated as $\frac{\alpha}{\beta}$, so possible prior distributions would be $\Gamma(1,1)$ and $\Gamma(0.5,1)$. The distributions are plotted below. These distributions work well in the exponential model and do not restrict the model flexibility too much.

```{r, echo=FALSE, out.width="60%", out.height="30%", fig.cap="Piror distributions", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/gamma_priors.png'))
``` 

For the three biomarkers, we are fitting a Gaussian model. It is important to note that we are fitting a similar model to each of the protein LYVE1, REG1B and TFF1 but they are not assumed to be connected to each other. 

We need prior distributions for the mean value and for the standard deviation. The prior of the mean is chosen so that it is sensible but not restricting. We have seen that the values of all three proteins obtain both negative and positive values after logarithmic transformation. This is why we will choose the mean of the prior of the parameter $\mu$ to be 0. To be able to easily adjust to the data, we will choose a relatively large value of 20 for the standard deviation. Hence, the prior distribution for the mean is $\mathcal{N}(0,20)$. In sensitivity analysis, we will notice that the choice of the standard deviation does not affect the result strongly, as long as it is wide enough. The prior distribution for $\sigma$ is chosen as $\Gamma(1,1)$ so that it stays positive and gives a wide enough range of values.

For the hierarchical model, we need to define prior distributions for the hyperparameters. Here we use same logic as above and choose prior distributions for all the hyperparameters that are required to be positive as $\Gamma(1,1)$ and hyperparameter of the mean of the normal distribution as $\mathcal{N}(0,20)$. These priors provide enough flexibility to fit models to the different diagnosis groups. 

# 5 Stan models

This chapter is reserved only for including comprehensive versions of the Stan models used in this study. They are otherwise complete, but computations for two of the three test subject groups have been omitted from the `generated quantities` blocks for the sake of saving space, since the logic is equivalent for each group. To see the complete Stan code, see appendix A.

## Pooled model

```{eval="FALSE"}
data {
  // Lengths of the observation vectors
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  // Observations of the 4 protein levels for each group
  vector[4] y1[N1];
  vector[4] y2[N2];
  vector[4] y3[N3];
}

transformed data {
  vector[3] log_y1[N1];
  vector[3] log_y2[N2];
  vector[3] log_y3[N3];
  
  for (j in 1:3){
    log_y1[,j] = log(y1[,j+1]);
    log_y2[,j] = log(y2[,j+1]);
    log_y3[,j] = log(y3[,j+1]);
  }
}

parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  vector [3] mu;
  vector<lower=0> [3] sigma;
}

model {
  // Priors
  alpha ~ gamma(1, 1);
  beta ~ gamma(0.5, 1);  
  for (j in 1:3) {
    mu[j] ~ normal(0,20);
    sigma[j] ~ gamma(1, 1);
  }
  // Likelihoods
  y1[,1] ~ gamma(alpha, beta);
  y2[,1] ~ gamma(alpha, beta);
  y3[,1] ~ gamma(alpha, beta);
  for (j in 1:3) {
    log_y1[,j] ~ normal(mu[j], sigma[j]);
    log_y2[,j] ~ normal(mu[j], sigma[j]);
    log_y3[,j] ~ normal(mu[j], sigma[j]);
  }
}

generated quantities {
  // Posterior predictive distributions
  vector[4] ypred_1;
  vector[4] ypred_2;
  vector[4] ypred_3;
  // Log-likelihoods of the posterior draws
  vector[4] log_lik_1[N1];
  vector[4] log_lik_2[N2];
  vector[4] log_lik_3[N3];
  
  // Group 1
  ypred_1[1] = gamma_rng(alpha, beta);
  for (n in 1:N1)
    log_lik_1[n,1] = gamma_lpdf(y1[n,1] | alpha, beta);
    
  for (j in 1:3) {
    ypred_1[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N1) {
      log_lik_1[n,j+1] = normal_lpdf(log_y1[n,j] | mu[j], sigma[j]);
    }
  }
  
  //...
}

```

## Hierarchical model

```{eval="FALSE"}
data {
  // Lengths of the observation vectors
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  // Observations of the 4 protein levels for each group
  vector[4] y1[N1];
  vector[4] y2[N2];
  vector[4] y3[N3];
}

transformed data {
  vector[3] log_y1[N1];
  vector[3] log_y2[N2];
  vector[3] log_y3[N3];
  
  for (j in 1:3){
    log_y1[,j] = log(y1[,j+1]);
    log_y2[,j] = log(y2[,j+1]);
    log_y3[,j] = log(y3[,j+1]);
  }
}

parameters {
  // Hyperparamters
  real<lower=0> alphaP[2];
  real<lower=0> betaP[2];
  vector[3] muP[2];
  vector<lower=0>[3] sigmaP[2];
  // Parameters
  real<lower=0> alpha[3];
  real<lower=0> beta[3];
  vector [3] mu[3];
  vector<lower=0> [3] sigma[3];
}

model {
  // k-> number of parameters in priors
  // j-> number of proteins
  // Hyperpriors
  for (k in 1:2){
    alphaP[k] ~ gamma(1,1);
    betaP[k] ~ gamma(1,1);
    sigmaP[k,] ~ gamma(1,1);
  }
  muP[1,] ~ normal(0,20);
  muP[2,] ~ gamma(1,1);
  // Priors
  alpha ~ gamma(alphaP[1], betaP[1]);
  beta ~ gamma(alphaP[2], betaP[2]);
  for (j in 1:3){
    mu[,j] ~ normal(muP[1,j], sigmaP[1,j]);
    sigma[,j] ~ gamma(muP[2,j], sigmaP[2,j]);
  }
  // Likelihood
  y1[,1] ~ gamma(alpha[1], beta[1]);
  y2[,1] ~ gamma(alpha[2], beta[2]);
  y3[,1] ~ gamma(alpha[3], beta[3]);
  for (j in 1:3){
    log_y1[,j] ~ normal(mu[1,j], sigma[1,j]);
    log_y2[,j] ~ normal(mu[2,j], sigma[2,j]);
    log_y3[,j] ~ normal(mu[3,j], sigma[3,j]);
  }
}

generated quantities {
  // Posterior predictive distributions
  vector[4] ypred_1;
  vector[4] ypred_2;
  vector[4] ypred_3;
  // Log-likelihoods of the posterior draws
  vector[4] log_lik_1[N1];
  vector[4] log_lik_2[N2];
  vector[4] log_lik_3[N3];
  
  // Group 1
  ypred_1[1] = gamma_rng(alpha[1], beta[1]);
  for (n in 1:N1)
    log_lik_1[n,1] = gamma_lpdf(y1[n,1] | alpha[1], beta[1]);
    
  for (j in 1:3) {
    ypred_1[j+1] = normal_rng(mu[1,j], sigma[1,j]);
    for (n in 1:N1) {
      log_lik_1[n,j+1] = normal_lpdf(log_y1[n,j] | mu[1,j], sigma[1,j]);
    }
  }
  
  //...
}

```

# 6 Fitting and convergence diagnostics

Both of the models were first run with the default options of the `rstan::stan()` function, that is

- Number of chains: 4
- Chain length: 2000
- Warmup length: floor(chain length / 2)

Below is part of the output (due to generation of log-likelihoods, the whole output has over 2000 rows) of `monitor(pooled_fit)`:

```{r, echo=FALSE}
pooled_res <- read.csv('./diagnostic_data/pooled_res.csv')
print(pooled_res[c(1:8),c(1:5)])
```

As we can see from the output, the pooled model seems to converge excellently with the default options. There are zero divergences, all $\hat{R}$ values differ from 1 by no more than roughly $10^{-3}$, and the effective sample sizes are above 3000. Hence, there was no need to tweak the options in this case. 

The fitting of the hierarchical model with default options raises a warning of a bit over 200 divergent post-warmup transitions Our first attempt to get that number down was to double the chain length, which seemed to have at least some positive effect. Then we lowered the sampling step size by setting the `adapt_delta` parameter to 0.99. This had a very good effect, and we were able to get the number of divergences to under 20. At this point, we decided to accept the result. Below is an example code block, showing how the hierarchical model was run. For the full script, see appendix B.

```{r, eval=FALSE}
hier_fit <- stan(
  file = 'path/to/repository/cancer-biomarker-analysis/models/hierarchical.stan',
  data = stan_data,
  iter = 4000,
  control = list(adapt_delta = 0.99)
)
```

A similar glance to the monitor output, as with the pooled model, yields the following:

```{r, echo=FALSE}
hier_res <- read.csv('./diagnostic_data/hier_res.csv')
print(hier_res[c(17:40),c(1:5)])
```

Judging by the effective sample sizes and $\hat{R}$ values, the hierarchical model seems to converge even better than the pooled one (after the corrections to the sampling options). However, we need to keep in mind the divergence warning we were not able to get rid of completely. All of the model parameter estimations show neat convergence, so we can conclude that some of the log-likelihood estimations diverge.

# 7 Posterior predictive checks

Posterior predictive checks were completed visually in this project. The Stan models include a generation of predictive samples for each diagnosis group and protein. In general, the posterior predictive check was done as follows: for both models, the same number of samples as in the original dataset was drawn four times. These samples were plotted as histograms, so that the distribution can be compared to the original histogram. Below you can find the figures. The original data is plotted as white histograms on the background and colored histograms present the draws.

For the pooled model, we are using the same parameters for each diagnosis group. Therefore, it is relevant to look at the data of each protein as one entity. 

We can see that for creatinine and REG1B, replicated samples correspond to the original data quite well. For REG1B, the normal distribution cannot present the longer tail on the left that the actual data has. However, the overall fit is satisfying. 

LYVE1 and TFF1 cause more reason to worry since the original data, after log-transformation, has two peaks meaning that the actual distribution would be bimodal. The normal distribution does not replicate well compared to the data because it is rather a compromise between these two peaks. However, the pooled model is already a combination of all of the diagnosis groups, so it could be expected that the fit to the data is not as good.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for the pooled model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_pooled_creatinine.png','./images/post_check_pooled_LYVE1.png', './images/post_check_pooled_REG1B.png', './images/post_check_pooled_TFF1.png'))
``` 

For the hierarchical model, we obtain overall twelve figures that are presented below, grouped by the diagnosis. Similarly as in the pooled model, replicated datasets for creatinine and REG1B correspond well to the original data for each diagnosis group. The same issue for REG1B can be seen here, as the skewness of the data cannot be taken into account with the normal distribution. This is a clear improvement point but especially for diagnosis group 3, the fit is actually quite good and covers all the extreme values.

For LYVE2 and TFF1, the same issue with two peaks in the data can be seen, especially with diagnosis groups 1 and 2. It is interesting to see that in diagnosis group 1 with healthy people, there are two clear peaks in the data. For diagnosis group 2, the skewness of a bimodal distribution makes the normal distribution a very bad choice, because the fitted distribution needs to be very wide. For protein LYVE1 and diagnosis group 3, the fit is actually quite good even though there are some extreme values on the left, that the normal distribution cannot replicate. 

For TFF1, the issues are very similar but the other peak is not as large, so the issues are not as strong. In general, the replicate data is better but there is still definitely room for improvement. It might be a good idea to fit a bimodal distribution instead of normal distribution and this will be discussed further in chapter 11.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for diagnosis 2 and the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_hier_creatinine_1.png','./images/post_check_hier_LYVE1_1.png', './images/post_check_hier_REG1B_1.png', './images/post_check_hier_TFF1_1.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for diagnosis 2 and the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_hier_creatinine_2.png','./images/post_check_hier_LYVE1_2.png', './images/post_check_hier_REG1B_2.png', './images/post_check_hier_TFF1_2.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for diagnosis 3 and the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_hier_creatinine_3.png','./images/post_check_hier_LYVE1_3.png', './images/post_check_hier_REG1B_3.png', './images/post_check_hier_TFF1_3.png'))
``` 

# 8 Model comparison

In the Stan models, we generate the log-likelihoods for the posterior draws, which are required in order to perform Leave one out -cross validation with Pareto-smoothed importance sampling (PSIS-LOO) to compare the two models (pooled and hierarchical). After extracting the log-likelihoods from the Stan fit, we proceed to perform PSIS-LOO using the `loo` package, store the results in the `diagnostics` matrix, and plot the pareto-k values. In the case of the hierarchical model, the procedure looks like this:

```{r, eval=FALSE}
# Initialize the diagnostics matrix
diagnostics <- matrix(0, nrow = 3, ncol = 2,
                      dimnames = list(
                        c('Group 1', 'Group 2', 'Group 3'),
                        c('ELPD', 'P_eff')
                      )
)

# Compute diagnostics for each group
for (i in 1:3) {
  log_lik <- log_liks[[i]]
  r_eff <- relative_eff(exp(log_lik)) # Relative efficiency
  loo <- loo(log_lik, r_eff=r_eff) # LOO object
  
  estimates <- loo$estimates
  elpd <- estimates[1,1] # PSIS-LOO value
  p_eff <- estimates[2,1] # Effective number of parameters
  
  diagnostics[i,1] = elpd
  diagnostics[i,2] = p_eff
  print(loo, plot_k = TRUE)
}
```

Below are the PSIS-LOO ELPD values and effective numbers of parameters for the pooled and hierarchical model, respectively.

```{r, echo=FALSE}
pooled_eval <- read.csv('./diagnostic_data/pooled_eval.csv')
hier_eval <- read.csv('./diagnostic_data/hier_eval.csv')
print(pooled_eval)
print(hier_eval)
```

To evaluate how reliable our PSIS-LOO evaluations are, we need to take a look at all the respective pareto-k values.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Pareto-k values for each test subject group in the pooled model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/pooled1_pareto_k.png','./images/pooled2_pareto_k.png', './images/pooled3_pareto_k.png'))
```

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Pareto-k values for each test subject group in the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/hier1_pareto_k.png','./images/hier2_pareto_k.png', './images/hier3_pareto_k.png'))
``` 

The k-values for the pooled model are excellent, there are no values above 0.5. Therefore, the ELPD-estimate in this case can be considered reliable. In the hierarchical case, the k-values seem to be in order as well, except for a single bad value in both group 1 and 3. This may be due to the divergences mentioned in chapter 6, since we concluded that the problems lie with the posterior draw log-likelihoods, which are utilized by PSIS-LOO. However, we concluded that a single bad pareto-k value is an uncertainty we can accept. It just needs to be taken into account, when using the results of our model comparison.

Assuming the PSIS-LOO estimates to be reliable, we can thus conclude that the hierarchical model should be preferred over the pooled one, since its ELPD-estimates are considerably higher.

# 9 Predictive performance assessment

This model is not applicable for predictive assessment since it does not use predictors but rather describes the distributions and their differences. The only predictive aspect of the model might be predicting the protein levels of a person based on their diagnosis. But since we are not using any other predictors at this point, it is not relevant or interesting. The results of this model could be used to create a predictive model, where the biomarkers are used to predict the occurrence of PDAC. These results could be used as guidance on how the predictive model could be built, and what kind of dependences there are between PDAC and the biomarkers.

# 10 Sensitivity analysis

Sensitivity analysis was carried out by comparing six different scenarios of priors for both models. In both cases, the different resulting posterior distributions were plotted and values of the Stan monitor checked. Sensitivity analysis was restricted to varying the parameters of the chosen prior distributions.

For the pooled model, the used scenarios are presented below in a table. In the first three scenarios, parameter values are varied only a little. The last three scenarios are extreme cases were the values have been changed drastically. 

```{r echo = FALSE, results = 'asis'}
library(knitr)
S <- matrix(c(1,1,0.5,1,0,20,1,1,
             2,2,1,2,0,10,2,2,
             0.5,0.5,0.1,0.5,0,2,0.5,0.5,
             0.01,0.01,0.01,0.01,10,1,0.01,0.01,
             100,100,100,100,1000,1000,100,100,
             100,0.01,100,0.01,-1000,1,100,0.01), ncol = 8, byrow = T)
df <- data.frame(S)
colnames(df) <- c('Shape of alpha', 'Scale of alpha','Shape of beta', 'Scale of beta',
                  'Mean of mu^j', 'Deviation of mu^j','Shape of sigma^j', 'Scale of sigma^j')
kable(df, caption='Scenarios for sensitivity analysis for pooled model',format.args = list(scientific = FALSE))
```

By looking at the figures, we can see that only the scenarios with very large variation in the prior parameters differ seemingly from the others. For all proteins, only scenarios 5 and 6 are separate from other results. For creatinine, the changes are visible but insignificant. For the three biomarkers, only scenario 6 really fails to create a similar posterior distribution. The reason is that the prior of the mean value of the normal distribution is very narrow, and we can safely say that $\mathcal{N}(-1000,1)$ is an extremely unrealistic option.

Overall, we can conclude that the model is very robust and not sensitive when varying the parameters of the prior distributions. In addition, the pooled model was fitted smoothly with all of the prior choices, which was not the case with the hierarchical model, as can be seen below.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Sensitivity analysis for the pooled model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/SA_pooled_creatinine.png','./images/SA_pooled_LYVE1.png', './images/SA_pooled_REG1B.png', './images/SA_pooled_TFF1.png'))
``` 

The same way, we created six scenarios for hierarchical model. Scenarios are listed below in the table. The posterior distributions have been plotted so that each scenario has its own color, and each diagnosis group has a different line type. Diagnosis 1 is plotted as a dashed line, diagnosis 2 as a double dashed line, and diagnosis 3 as a solid line.

```{r echo = FALSE, results = 'asis'}
library(knitr)
S <- matrix(c(1,1,1,1,1,1,0,20,1,1,
              2,2,2,2,2,2,0,10,2,2,
              0.5,0.5,0.5,0.5,0.5,0.5,0,2,0.5,0.5,
              0.01,0.01,0.01,0.01,0.01,0.01,10,1,0.01,0.01,
              100,100,100,100,100,100,1000,1000,100,100,
              100,0.01,100,0.01,100,0.01,-1000,1,100,0.01), ncol = 10, byrow = T)
df <- data.frame(S)
colnames(df) <- c('Shape of alpha_Pk', 'Scale of alpha_Pk','Shape of beta_Pk', 'Scale of beta_Pk',
                  'Mean of mu_P1^j', 'Deviation of mu_P1^j','Shape of mu_P2^j', 'Scale of mu_P2^j',
                  'Shape of sigma_Pk^j', 'Scale of sigma_Pk^j')
kable(df, caption='Scenarios for sensitivity analysis for pooled model',format.args = list(scientific = FALSE))
```

Similarly as in the pooled model, the posterior distribution of creatinine stays quite the same for all the priors but scenarios 5 and 6 are somewhat separate from the others for each diagnosis group. The same kind of behavior can be seen for other proteins as well, and even the scenario 6 can be fitted better than in the pooled model. We can conclude that the hierarchical model is not sensitive to the choice of prior parameters since only the unrealistically large changes can be seen in the resulting posterior distributions.

On the other hand, it is noteworthy that even though the results end up to be rather good with strange prior distributions, the fitting of the model does not go as smoothly. A lot of problems occur when initializing the Metropolis algorithm in scenarios 4, 5 and 6. This slows down the computations and might affect other diagnostics of the model. Therefore it is better to use more realistic priors.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Sensitivity analysis for the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/SA_hier_creatinine.png','./images/SA_hier_LYVE1.png', './images/SA_hier_REG1B.png', './images/SA_hier_TFF1.png'))
``` 

# 11 Issues and potential improvements

As discussed in chapter 7, when making posterior predictive checks, there are multiple possibilities to improve the model. For creatinine, the model works quite well, but for the three biomarkers, the Gaussian model could be changed to something more flexible. For example, for REG1B, a normal distribution fits quite well, but a skewed distribution could take into account extreme values better. Data could be transformed to a positive scale so that, e.g. a log-normal distribution could be used.    

Secondly, we can see that log-transformed LYVE1 and TFF1 data has two peaks which would imply that the data is bimodally distributed. This is an interesting question, since two peaks can be seen within healthy individuals. The reasons for this could be investigated further, so that we can conclude whether the explanation is lack of data, or that there actually can be observed two kinds of individuals. 

Depending on the conclusion, one might consider fitting a bimodal distribution or a mixture of several distributions. On the other hand, if there are attributes that predict the values of LYVE1 and TFF1, these could be used to create a model that corresponds to the data better. For example, after investigating the data, there seems to be a dependence between creatinine levels and LYVE1 and TFF1. Trying to fit a bimodal distribution to the current data might work, but it could result in overfitting, since the number of observations might not be large enough.

As we concluded in chapter 8, model comparison suggests quite strongly to prefer the hierarchical model over the pooled one, even though there are some uncertainties (see detailed discussion in chapter 8). As it seems likely that the cause of said uncertainties lies with some divergences during MCMC, another topic for improvement would be to fix those divergences. This could be achieved by e.g. reparameterization of the hierarchical model using Neal's funnel, as suggested in the Stan documentation.[4]  

# 12 Conclusion

To be able to evaluate the results of the data analysis, the resulting posterior distribution for each biomarker has been plotted below. Results from the pooled and the hierarchical model are presented side by side, even though our study suggests that the latter outperforms the former. We can see that creatinine has only small differences between different diagnosis groups. Therefore it cannot be used as an indicator for PDAC, which was already made obvious in the original research.[2]

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of creatinine data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and read diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_creatinine.png','./images/post_dist_hier_creatinine.png'))
``` 

For the three biomarkers, the results between pooled and hierarchical model differ significantly, and we can conclude that there are differences between the diagnosis groups, at least based on this dataset. Note that green color corresponds to the healthy control group, blue color to the group with some benign pancreatic disease, and red color to the patients with positive PDAC diagnosis.

When examining the figures for LYVE1, we can see that diagnosis groups 1 and 3 have very wide distributions, but they are somewhat separated. Diagnosis group 2 has a quite narrow distribution, but it overlaps entirely with group 3. This implies that LYVE1 can be used as an indicator for pancreatic disease but conclusions about PDAC are hard to make.

For REG1B, distributions for each diagnosis group are very similar by shape, and for diagnosis group 1, the distribution is very wide. Once again, the distributions of groups 2 and 3 overlap, but now the largest values of REG1B indicate PDAC. REG1B could be then used as an indicator or predictor for PDAC, but differentiating from other (benign) pancreatic diseases might be difficult.

For TFF1, the distribution for the control group is the narrowest, so this biomarker is best for recognizing healthy people. The overlap between groups 2 and 3 is small, so large values of TFF1 imply some kind of pancreatic disease. Again, in the case of TFF1, it is difficult to separate PDAC from other diseases and the distribution of group 3 is very wide.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of LYVE1 data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and red diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_LYVE1.png','./images/post_dist_hier_LYVE1.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of REG1B data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and red diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_REG1B.png','./images/post_dist_hier_REG1B.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of TFF1 data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and red diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_TFF1.png','./images/post_dist_hier_TFF1.png'))
``` 

Finally, it has to be stated, that even though our hierarchical model seems to perform quite well and yields comprehensive results, it still requires improvement (as suggested in chapter 11) before we recommend it to be applied in context. That being said, we conclude this study to be a suitable basis for further research and development to be conducted in the area of PDAC prediction using Bayesian inference.  

# 13 Reflection

Conducting this kind of project work has been an efficient way of tying all the knowledge and methods we have learned during this course into a comprehensive package. It finally clarified the big picture of Bayesian data analysis and its workflow, from choosing the right distribution shapes and priors all the way to model evaluation and comparison. It also gave us a deeper understanding of several sub-topics, which the weekly assignments of the course touched on only superficially. Below are some of them:

- The need for transforming the data: when to apply e.g. log transformations and how to do it in practice
- Choosing models and distributions (in the weekly assignments, these were always given)
- Convergence diagnostics, and what tools are available if everything does not converge the first time
- Posterior predictive checking (this study was a first deep dive into the topic for us)

All in all, we are very satisfied with our work. It has been an interesting, challenging, and rewarding journey, and we feel that the meaningfulness of our topic also gave us more than enough motivation to see it through properly.

# 14 References

- [1] Sarantis P, Koustas E, Papadimitropoulou A, Papavassiliou AG, Karamouzis MV. (2020) *Pancreatic ductal adenocarcinoma: Treatment hurdles, tumor microenvironment and immunotherapy.* World J Gastrointest Oncol. 2020;12(2):173-181. https://doi.org/10.4251/wjgo.v12.i2.173
- [2] Debernardi S, O'Brien H, Algahmdi AS, Malats N, Stewart GD, Plješa-Ercegovac M, et al. (2020) *A combination of urinary biomarker panel and PancRISK score for earlier detection of pancreatic cancer: A case--control study.* PLoS Med 17(12): e1003489. https://doi.org/10.1371/journal.pmed.1003489
- [3] Bayesian Data Analysis, Third edition; Gelman, Carlin, Stern, Dunson, Vehtari and Rubin; 2016
- [4] https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html

# 15 Appendices

Included is only the code that is referred to in this report. For the complete project repository, see https://github.com/jHimanen/cancer-biomarker-analysis.

## Appendix A: complete Stan models

### Pooled

```{eval=FALSE}
data {
  // Lengths of the observation vectors
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  // Observations of the 4 protein levels for each group
  vector[4] y1[N1];
  vector[4] y2[N2];
  vector[4] y3[N3];
}

transformed data {
  vector[3] log_y1[N1];
  vector[3] log_y2[N2];
  vector[3] log_y3[N3];
  
  for (j in 1:3){
    log_y1[,j] = log(y1[,j+1]);
    log_y2[,j] = log(y2[,j+1]);
    log_y3[,j] = log(y3[,j+1]);
  }
}

parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  vector [3] mu;
  vector<lower=0> [3] sigma;
}

model {
  // Priors
  alpha ~ gamma(1, 1);
  beta ~ gamma(0.5, 1);  
  for (j in 1:3) {
    mu[j] ~ normal(0,20);
    sigma[j] ~ gamma(1, 1);
  }
  // Likelihoods
  y1[,1] ~ gamma(alpha, beta);
  y2[,1] ~ gamma(alpha, beta);
  y3[,1] ~ gamma(alpha, beta);
  for (j in 1:3) {
    log_y1[,j] ~ normal(mu[j], sigma[j]);
    log_y2[,j] ~ normal(mu[j], sigma[j]);
    log_y3[,j] ~ normal(mu[j], sigma[j]);
  }
}

generated quantities {
  // Posterior predictive distributions
  vector[4] ypred_1;
  vector[4] ypred_2;
  vector[4] ypred_3;
  // Log-likelihoods of the posterior draws
  vector[4] log_lik_1[N1];
  vector[4] log_lik_2[N2];
  vector[4] log_lik_3[N3];
  
  // Group 1
  ypred_1[1] = gamma_rng(alpha, beta);
  for (n in 1:N1)
    log_lik_1[n,1] = gamma_lpdf(y1[n,1] | alpha, beta);
    
  for (j in 1:3) {
    ypred_1[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N1) {
      log_lik_1[n,j+1] = normal_lpdf(log_y1[n,j] | mu[j], sigma[j]);
    }
  }
  //Group 2
  ypred_2[1] = gamma_rng(alpha, beta);
  for (n in 1:N2)
    log_lik_2[n,1] = gamma_lpdf(y2[n,1] | alpha, beta);
    
  for (j in 1:3) {
    ypred_2[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N2) {
      log_lik_2[n,j+1] = normal_lpdf(log_y2[n,j] | mu[j], sigma[j]);
    }
  }
  //Group 3
  ypred_3[1] = gamma_rng(alpha, beta);
  for (n in 1:N3)
      log_lik_3[n,1] = gamma_lpdf(y3[n,1] | alpha, beta);
      
  for (j in 1:3) {
    ypred_3[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N3) {
      log_lik_3[n,j+1] = normal_lpdf(log_y3[n,j] | mu[j], sigma[j]);
    }
  }
}

```

### Hierarchical

```{eval=FALSE}
data {
  // Lengths of the observation vectors
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  // Observations of the 4 protein levels for each group
  vector[4] y1[N1];
  vector[4] y2[N2];
  vector[4] y3[N3];
}

transformed data {
  vector[3] log_y1[N1];
  vector[3] log_y2[N2];
  vector[3] log_y3[N3];
  
  for (j in 1:3){
    log_y1[,j] = log(y1[,j+1]);
    log_y2[,j] = log(y2[,j+1]);
    log_y3[,j] = log(y3[,j+1]);
  }
}

parameters {
  // Hyperparamters
  real<lower=0> alphaP[2];
  real<lower=0> betaP[2];
  vector[3] muP[2];
  vector<lower=0>[3] sigmaP[2];
  // Parameters
  real<lower=0> alpha[3];
  real<lower=0> beta[3];
  vector [3] mu[3];
  vector<lower=0> [3] sigma[3];
}

model {
  // k-> number of parameters in priors
  // j-> number of proteins
  // Hyperpriors
  for (k in 1:2){
    alphaP[k] ~ gamma(1,1);
    betaP[k] ~ gamma(1,1);
    sigmaP[k,] ~ gamma(1,1);
  }
  muP[1,] ~ normal(0,20);
  muP[2,] ~ gamma(1,1);
  // Priors
  alpha ~ gamma(alphaP[1], betaP[1]);
  beta ~ gamma(alphaP[2], betaP[2]);
  for (j in 1:3){
    mu[,j] ~ normal(muP[1,j], sigmaP[1,j]);
    sigma[,j] ~ gamma(muP[2,j], sigmaP[2,j]);
  }
  // Likelihood
  y1[,1] ~ gamma(alpha[1], beta[1]);
  y2[,1] ~ gamma(alpha[2], beta[2]);
  y3[,1] ~ gamma(alpha[3], beta[3]);
  for (j in 1:3){
    log_y1[,j] ~ normal(mu[1,j], sigma[1,j]);
    log_y2[,j] ~ normal(mu[2,j], sigma[2,j]);
    log_y3[,j] ~ normal(mu[3,j], sigma[3,j]);
  }
}

generated quantities {
  // Posterior predictive distributions
  vector[4] ypred_1;
  vector[4] ypred_2;
  vector[4] ypred_3;
  // Log-likelihoods of the posterior draws
  vector[4] log_lik_1[N1];
  vector[4] log_lik_2[N2];
  vector[4] log_lik_3[N3];
  
  // Group 1
  ypred_1[1] = gamma_rng(alpha[1], beta[1]);
  for (n in 1:N1)
    log_lik_1[n,1] = gamma_lpdf(y1[n,1] | alpha[1], beta[1]);
    
  for (j in 1:3) {
    ypred_1[j+1] = normal_rng(mu[1,j], sigma[1,j]);
    for (n in 1:N1) {
      log_lik_1[n,j+1] = normal_lpdf(log_y1[n,j] | mu[1,j], sigma[1,j]);
    }
  }
  //Group 2
  ypred_2[1] = gamma_rng(alpha[2], beta[2]);
  for (n in 1:N2)
    log_lik_2[n,1] = gamma_lpdf(y2[n,1] | alpha[2], beta[2]);
    
  for (j in 1:3) {
    ypred_2[j+1] = normal_rng(mu[2,j], sigma[2,j]);
    for (n in 1:N2) {
      log_lik_2[n,j+1] = normal_lpdf(log_y2[n,j] | mu[2,j], sigma[2,j]);
    }
  }
  //Group 3
  ypred_3[1] = gamma_rng(alpha[3], beta[3]);
  for (n in 1:N3)
      log_lik_3[n,1] = gamma_lpdf(y3[n,1] | alpha[3], beta[3]);
      
  for (j in 1:3) {
    ypred_3[j+1] = normal_rng(mu[3,j], sigma[3,j]);
    for (n in 1:N3) {
      log_lik_3[n,j+1] = normal_lpdf(log_y3[n,j] | mu[3,j], sigma[3,j]);
    }
  }
}

```

## Appendix B: R script for fitting, diagnostics and posterior predictive checks in case of the hierarchical model

```{r, eval=FALSE}
library(rstan)
library(loo)
library(ggplot2)

# REPLACE WITH WORKING PATH
data_path <- '.../Debernardi et al 2020 data.csv'
data <- read.csv(data_path)

# Split the data into test subject groups
group_1 <- data[data$diagnosis == 1,]
group_2 <- data[data$diagnosis == 2,]
group_3 <- data[data$diagnosis == 3,]

# Organize input data for the Stan model 
stan_data <- list(
  N1 = length(group_1[,1]),
  N2 = length(group_2[,1]),
  N3 = length(group_3[,1]),
  y1 = group_1[, c('creatinine', 'LYVE1', 'REG1B', 'TFF1')],
  y2 = group_2[, c('creatinine', 'LYVE1', 'REG1B', 'TFF1')],
  y3 = group_3[, c('creatinine', 'LYVE1', 'REG1B', 'TFF1')]
)

# Fit the Stan model
hier_fit <- stan(
  # REPLACE WITH WORKING PATH
  file = '.../cancer-biomarker-analysis/models/hierarchical.stan',
  data = stan_data,
  iter = 4000,
  control = list(adapt_delta = 0.99)
)

# Store convergence diagnostics
results <- monitor(hier_fit)
selected_res <- results[c(1:40), c('mean', 'sd', 'n_eff', 'Rhat', 'Q5', 'Q50', 'Q95')]
res_df <- as.data.frame(selected_res)
# REPLACE WITH WORKING PATH
write.csv(res_df,'.../cancer-biomarker-analysis/diagnostic_data/hier_res.csv')

# Extract log-likelihoods for LOO evaluation
log_liks <- list(
  extract_log_lik(hier_fit, parameter_name = 'log_lik_1', merge_chains = FALSE),
  extract_log_lik(hier_fit, parameter_name = 'log_lik_2', merge_chains = FALSE),
  extract_log_lik(hier_fit, parameter_name = 'log_lik_3', merge_chains = FALSE)
)

# Initialize the diagnostics matrix
diagnostics <- matrix(0, nrow = 3, ncol = 2,
                      dimnames = list(
                        c('Group 1', 'Group 2', 'Group 3'),
                        c('ELPD', 'P_eff')
                      )
)

# Compute diagnostics for each group
for (i in 1:3) {
  log_lik <- log_liks[[i]]
  r_eff <- relative_eff(exp(log_lik)) # Relative efficiency
  loo <- loo(log_lik, r_eff=r_eff) # LOO object
  
  estimates <- loo$estimates
  elpd <- estimates[1,1] # PSIS-LOO value
  p_eff <- estimates[2,1] # Effective number of parameters
  
  diagnostics[i,1] = elpd
  diagnostics[i,2] = p_eff
  print(loo, plot_k = TRUE)
}

hier_eval <- as.data.frame(diagnostics)
# REPLACE WITH WORKING PATH
write.csv(hier_eval, '.../cancer-biomarker-analysis/diagnostic_data/hier_eval.csv')

# Visualize posterior distributions

draws <- as.data.frame(hier_fit)

pCreatinine <- ggplot() + ggtitle('Posterior distribution of creatinine')
x <- seq(0,5,0.05)
for (i in seq(1,8000,100)) {
  df1 <- data.frame(x=x,y=dgamma(x,draws$`alpha[1]`[i],draws$`beta[1]`[i]))
  df2 <- data.frame(x=x,y=dgamma(x,draws$`alpha[2]`[i],draws$`beta[2]`[i]))
  df3 <- data.frame(x=x,y=dgamma(x,draws$`alpha[3]`[i],draws$`beta[3]`[i]))
  pCreatinine <- pCreatinine + 
    geom_line(data=df1,aes(x,y), color="lightgreen",alpha=0.4) +    
    geom_line(data=df2,aes(x,y), color="skyblue",alpha=0.4) +
    geom_line(data=df3,aes(x,y), color="tomato",alpha=0.4)
}
pCreatinine

pLYVE1 <- ggplot() + ggtitle('Posterior distribution of LYVE1')
x <- seq(-15,15,0.1)
for (i in seq(1,8000,100)) {
  df1 <- data.frame(x=x,y=dnorm(x,draws$`mu[1,1]`[i],draws$`sigma[1,1]`[i]))
  df2 <- data.frame(x=x,y=dnorm(x,draws$`mu[1,2]`[i],draws$`sigma[1,2]`[i]))
  df3 <- data.frame(x=x,y=dnorm(x,draws$`mu[1,3]`[i],draws$`sigma[1,3]`[i]))
  pLYVE1 <- pLYVE1 + 
    geom_line(data=df1,aes(x,y),col='lightgreen',alpha=0.4) +    
    geom_line(data=df2,aes(x,y),col='skyblue',alpha=0.4) +
    geom_line(data=df3,aes(x,y),col='tomato',alpha=0.4)
}
pLYVE1

pREG1B <- ggplot() + ggtitle('Posterior distribution of REG1B')
x <- seq(-10,15,0.1)
for (i in seq(1,8000,100)) {
  df1 <- data.frame(x=x,y=dnorm(x,draws$`mu[2,1]`[i],draws$`sigma[2,1]`[i]))
  df2 <- data.frame(x=x,y=dnorm(x,draws$`mu[2,2]`[i],draws$`sigma[2,2]`[i]))
  df3 <- data.frame(x=x,y=dnorm(x,draws$`mu[2,3]`[i],draws$`sigma[2,3]`[i]))
  pREG1B <- pREG1B + 
    geom_line(data=df1,aes(x,y),col='lightgreen',alpha=0.4) +    
    geom_line(data=df2,aes(x,y),col='skyblue',alpha=0.4) +
    geom_line(data=df3,aes(x,y),col='tomato',alpha=0.4)
}
pREG1B

pTFF1 <- ggplot() + ggtitle('Posterior distribution of TFF1')
x <- seq(-3,15,0.1)
for (i in seq(1,8000,100)) {
  df1 <- data.frame(x=x,y=dnorm(x,draws$`mu[3,1]`[i],draws$`sigma[3,1]`[i]))
  df2 <- data.frame(x=x,y=dnorm(x,draws$`mu[3,2]`[i],draws$`sigma[3,2]`[i]))
  df3 <- data.frame(x=x,y=dnorm(x,draws$`mu[3,3]`[i],draws$`sigma[3,3]`[i]))
  pTFF1 <- pTFF1 + 
    geom_line(data=df1,aes(x,y),col='lightgreen',alpha=0.4) +    
    geom_line(data=df2,aes(x,y),col='skyblue',alpha=0.4) +
    geom_line(data=df3,aes(x,y),col='tomato',alpha=0.4)
}
pTFF1

# Posterior predictive check

# Visualizing posterior predictive distributions
stan_hist(hier_fit, pars = c('ypred_1[1]','ypred_1[2]','ypred_1[3]','ypred_1[4]'))
stan_hist(hier_fit, pars = c('ypred_2[1]','ypred_2[2]','ypred_2[3]','ypred_2[4]'))
stan_hist(hier_fit, pars = c('ypred_3[1]','ypred_3[2]','ypred_3[3]','ypred_3[4]'))

plotPostCheck <- function(dataCol, ypred, N, bin, title, xlabel){
  df<- data.frame(values=dataCol)
  p <- ggplot() + 
    geom_histogram(data=df,aes(x=values),fill='white',color="black", binwidth = bin) +
    ggtitle(title) + xlab(xlabel)
  for (j in 1:4) {
    df <- data.frame(values=sample(ypred,N))
    p <- p + geom_histogram(data=df,aes(x=values),fill=j+1, alpha = 0.2, binwidth = bin)
  }
  p
}

#Creatinine
pPostCheck_C1 <- plotPostCheck(data[data$diagnosis==1,]$creatinine,draws$`ypred_1[1]`,
                              stan_data$N1,0.1,
                              "Replicated datasets of creatinine
                              compared to the original data for diagnosis 1",
                              "creatinine")
pPostCheck_C1
pPostCheck_C2 <- plotPostCheck(data[data$diagnosis==2,]$creatinine,draws$`ypred_2[1]`,
                               stan_data$N2,0.1,
                               "Replicated datasets of creatinine
                               compared to the original data for diagnosis 2 ",
                               "creatinine")
pPostCheck_C2
pPostCheck_C3 <- plotPostCheck(data[data$diagnosis==3,]$creatinine,draws$`ypred_3[1]`,
                               stan_data$N3,0.1,
                               "Replicated datasets of creatinine
                               compared to the original data for diagnosis 3",
                               "creatinine")
pPostCheck_C3

#LYVE1
pPostCheck_L1 <- plotPostCheck(log(data[data$diagnosis==1,]$LYVE1),draws$`ypred_1[2]`,
                               stan_data$N1,0.5,
                               "Replicated datasets of LYVE1
                               compared to the original data for diagnosis 1",
                               "log(LYVE1)")
pPostCheck_L1
pPostCheck_L2 <- plotPostCheck(log(data[data$diagnosis==2,]$LYVE1),draws$`ypred_2[2]`,
                               stan_data$N2,0.5,
                               "Replicated datasets of LYVE1
                               compared to the original data for diagnosis 2",
                               "log(LYVE1)")
pPostCheck_L2
pPostCheck_L3 <- plotPostCheck(log(data[data$diagnosis==3,]$LYVE1),draws$`ypred_3[2]`,
                               stan_data$N3,0.5,
                               "Replicated datasets of LYVE1
                               compared to the original data for diagnosis 3",
                               "log(LYVE1)")
pPostCheck_L3

#REG1B
pPostCheck_R1 <- plotPostCheck(log(data[data$diagnosis==1,]$REG1B),draws$`ypred_1[3]`,
                               stan_data$N1,0.5,
                               "Replicated datasets of REG1B
                               compared to the original data for diagnosis 1",
                               "log(REG1B)")
pPostCheck_R1
pPostCheck_R2 <- plotPostCheck(log(data[data$diagnosis==2,]$REG1B),draws$`ypred_2[3]`,
                               stan_data$N2,0.5,
                               "Replicated datasets of REG1B
                               compared to the original data for diagnosis 2",
                               "log(REG1B)")
pPostCheck_R2
pPostCheck_R3 <- plotPostCheck(log(data[data$diagnosis==3,]$REG1B),draws$`ypred_3[3]`,
                               stan_data$N2,0.5,
                               "Replicated datasets of REG1B
                               compared to the original data for diagnosis 3",
                               "log(REG1B)")
pPostCheck_R3

#TFF1
pPostCheck_T1 <- plotPostCheck(log(data[data$diagnosis==1,]$TFF1),draws$`ypred_1[4]`,
                               stan_data$N1,1,
                               "Replicated datasets of TFF1
                               compared to the original data for diagnosis 1",
                               "log(TFF1)")
pPostCheck_T1
pPostCheck_T2 <- plotPostCheck(log(data[data$diagnosis==2,]$TFF1),draws$`ypred_2[4]`,
                               stan_data$N2,1,
                               "Replicated datasets of TFF1
                               compared to the original data for diagnosis 2",
                               "log(TFF1)")
pPostCheck_T2
pPostCheck_T3 <- plotPostCheck(log(data[data$diagnosis==3,]$TFF1),draws$`ypred_3[4]`,
                               stan_data$N3,1,
                               "Replicated datasets of TFF1
                               compared to the original data for diagnosis 3",
                               "log(TFF1)")
pPostCheck_T3
```
