---
title: 'A Bayesian approach to analyzing urinary biomarkers for pancreatic cancer'
author: "Noora Torpo, Joel Himanen"
date: "11/25/2021"
output:
  rmarkdown::pdf_document:
    fig_caption: yes        
    includes:  
      in_header: preamble-latex.tex
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1)  Introduction describing the motivation, the problem and the main modeling idea. Showing some illustrative figure is recommended. (J)
2)  Description of the data and the analysis problem. Provide information where the data was obtained, and if it has been previously used in some online case study and how your analysis differs from the existing analyses. (J)
3)  Description of at least two models, for example: non hierarchical and hierarchical, linear and non linear, variable selection with many models. Pooled (J); Hierarchical (N)
4)  Informative or weakly informative priors, and justification of their choices. (N)
5)  Stan, rstanarm or brms code.
6)  How the Stan model was run, that is, what options were used. This is also more clear as combination of textual explanation and the actual code line. (J)
7)  Convergence diagnostics (RË†, ESS, divergences) and what was done if the convergence was not good with the first try. (J)
8)  Posterior predictive checks and what was done to improve the model. (N)
9)  Model comparison (e.g. with LOO-CV). (J)
10) Predictive performance assessment if applicable (e.g. classification accuracy) and evaluation of practical usefulness of the accuracy. If not applicable, then explanation why in this case the predictive performance is not applicable. (N)
11) Sensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior is changed) (N)
12) Discussion of issues and potential improvements.
13) Conclusion what was learned from the data analysis.
14) Self-reflection of what the group learned while making the project.

# 1 Introduction

Pancreatic cancer, or more accurately, its most common form, *Pancreatic ductal adenocarcinoma* (PDAC), is one of the most lethal forms of cancer.
It is often diagnosed at a very late stage, which, combined to its heterogenous base of genetic mutations, leads to an average 5-year survival rate of less than 10%.[1] Therefore, research on early signs of PDAC can be considered to be of utmost importance.

This study focuses on conducting Bayesian inference on a data set, which holds measurements of potential biomarkers for PDAC.
We aim to model the biomarker levels using prior information to generate posterior distributions using two different approaches in Stan and R.
The goal is to see if and how the distributions for the different biomarkers differ between healthy and positively diagnosed test subjects.

# 2 Data and workflow

The data used in this study dates back to research done by Debernardi et al. in 20020.[2] The set includes information and measurements from 590 test subjects, the measurements considered in this study being amounts of four different proteins measured from urine samples:

-   **Creatinine**, commonly used to indicate kidney function
-   **LYVE1** (lymphatic vessel endothelial hyaluronan receptor 1), may have a part in tumor metastasis
-   **REG1B**, potentially related to pancreas regeneration
-   **TFF1** (trefoil factor 1), may be associated with regeneration and repair of the urinary tract

Of these, the three latter ones are considered target biomarkers, that could potentially help early diagnosis of PDAC.

Test subjects are divided into three different groups:

-   1 = Healthy control subjects (183 samples)
-   2 = Subjects with some beningn pancreatic disease (208 samples)
-   3 = Subjects with a positive PDAC diagnosis (199 samples)

Figure 1 is a visualization of these observations. Each histogram represents one of the four measured protein amounts. The subject groups are color coded (1 = green, 2 = blue, 3 = red), with overlaps presented as mixtures of the overlapping colors.  

```{r, echo=FALSE, out.width="60%", out.height="30%", fig.cap="Histograms of the observed data", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/creatinine_data.png','./images/lyve1_data.png', './images/reg1b_data.png', './images/tff1_data.png'))
``` 

The workflow of this study attempts to fit some probability distributions to each of the four protein levels in all three subject groups.
This is done by choosing weakly to somewhat informative priors for the parameters of these target distributions.
Using Stan, we then compute the likelihoods for our observations, and generate the resulting posterior distributions.
Our hypothesis is, that the posterior distributions of at least some of the biomarkers differ considerably between healthy and PDAC-positive groups.
We will also compare two different modeling approaches:

-   A pooled model, where we assume all three subject groups have the same prior distributions
-   A hierarchical model, where each prior has differently distributed hyperparameters

Data analysis in the original paper was done using "classical" statistical learning theory and tests.
To our knowledge, no other analyses have been performed on this dataset, so we can assume that this is the first time a Bayesian approach has been used on this case.

# 3 Models

In this problem setting, we are fitting two types of models for the data: a pooled model and a hierarchical model. The goal is to find distributions for proteins for all three diagnosis groups. Therefore, the input data includes three matrices including the data and three scalars indicating the number of observations. Note that the groups have different lengths. The data is denoted by $y_i$, $i=1,2,3$ corresponding to each group, where each column presents four different proteins. Similarly the number of observations is denote as $n_i$. 

For the pooled model, we are using same parameters for all of the groups meaning that, in practice, we are not considering the groups but the whole dataset. For hierarchical model, the distributions have shared hyperparameters but for different groups the distributions are fitted separately. 

As mentioned above, we are considering four proteins or biomarkers: creatinine, LYVE1, REG1B and TFF1. In both pooled and hierarchical model, the distributions are fitted separately for each protein. After considering the distributions of the data, we decided to use different types of models for creatinine and other proteins.

For creatinine data, an exponential model with two parameters is used. When considering the histogram in Part 2, we can see that exponential model is quite natural choice. In practice, this means that we are fitting a gamma distribution, as we are using mathematically more general formulation of exponential model as described in the Bayesian Data Analysis book [3].

For other three biomarkers: LYVE1, REG1B and TFF1, we first tried to use an exponential model but it turned out that it could not be fitted to the data sensibly. The differences between the largest and the smallest values are drastic and, especially, for REG1B and TFF1, the parameters obtain values that were practically zero. Therefore, we decided to make a logarithmic transformation to the data. The results can be seen in the histograms in Part 2. Note that we are using natural logarithm.

As some of the values are negative, we decided to simply use Gaussian model. Skewed distribution might be a better choice but the most obvious option such as log-normal distribution requires values to be positive. To avoid over fitting and unnecessary transformation, we decided to fit a Gaussian model for the three remaining proteins. In addition, after logarithmic transformation, the histograms of the three biomarkers were similar enough for us to use same prior distributions for their parameters making the implementation more straightforward.

## Pooled model

Exponential model fitted to the creatinine data is be written mathematically below. Note that the choice of priors will be discussed in Part 4. The first column of the data is denoted as $y_i^1$ corresponding to creatinine data for diagnosis group $i$. Gamma distribution is denoted as $\Gamma$.

$$
\begin{aligned}
y_i^1 &\sim \Gamma(\alpha, \beta) \hspace{1cm} i=1,2,3 \\
\alpha &\sim \Gamma(1,1) \\
\beta &\sim \Gamma(0.5,1)
\end{aligned}
$$

The Gaussian model fitted to the data for three other biomarkers is written mathematically below. Indexing $j=2,3,4$ refers to the proteins LYVE1, REB1B and TFF1 respectively.

$$
\begin{aligned}
ln(y_i^j) &\sim \mathcal{N}\left(\mu^j, {\sigma^j}^2\right) \hspace{1cm} i=1,2,3 \hspace{5mm} j=2,3,4 \\
\mu^j &\sim \mathcal{N}\left(0,20^2\right)  \\
\sigma^j &\sim \Gamma(1,1)  \\
\end{aligned}
$$

## Hierarchical model

Hierarchical model is constructed similarly but the prior distributions use hyperparameters and the actual parameters are specific for each diagnosis group. In general, we will denote hyperparameters with lower index $P$. The corresponding hierarchical formulation of the exponential model to the creatinine data can be written as follows.


$$
\begin{aligned}
y_i^1 &\sim \Gamma(\alpha_i, \beta_i) \hspace{1cm} i=1,2,3 \\
\alpha_i &\sim \Gamma(\alpha_{P_1},\beta_{P_1}) \\
\beta_i &\sim \Gamma(\alpha_{P_1},\beta_{P_1}) \\
\alpha_{P_k} &\sim \Gamma(1,1) \hspace{1cm} k=1,2 \\
\beta_{P_k} &\sim \Gamma(1,1)
\end{aligned}
$$

The corresponding hierarchical formulation of the Gaussian model fitted to the data for three other proteins can be written as follows.

$$
\begin{aligned}
ln(y_i^j) &\sim \mathcal{N}\left(\mu^j_i, {\sigma^j_i}^2\right) \hspace{1cm} &i=1,2,3 \hspace{5mm} j=2,3,4 \\
\mu^j_i &\sim \mathcal{N}\left(\mu_{P_1}^j,{\sigma_{P_1}^j}^2\right)   \\
\sigma^j_i &\sim \Gamma\left(\mu_{P_2}^j,\sigma_{P_2}^j\right) \\
\mu_{P_1}^j &\sim \mathcal{N}\left(0,20^2\right) \\
\mu_{P_2}^j &\sim \Gamma(1,1) \\
\sigma_{P_k}^j &\sim \Gamma(1,1) & k=1,2
\end{aligned}
$$

# 4 Priors

In general, the models required two kind of prior distributions. For parameters of gamma distribution, it is essentials to keep them positive which narrows the possible choice of priors. As presented in the course books Bayesian Data Analysis [3], a gamma distribution is a conjugate prior for exponential model. Therefore, gamma distribution is a natural choice for prior distributions for both parameters $\alpha$ and $\beta$ for creatinine. The gamma distribution is naturally quite narrow so we decided to consider informative priors but aim for enough flexibility in the model. 

The first parameter of gamma distribution $\alpha$ determines the shape of the distribution and the second parameter $\beta$ the scale. After considering creatinine data, we decided to roughly aim for expected value of 1 for the shape and expected value of 0.5 for the scale parameter. The expected value of a gamma distribution is calculated as $\frac{\alpha}{\beta}$ so possible prior distributions would be $\Gamma(1,1)$ and $\Gamma(0.5,1)$. The distributions are plotted below. These distributions work well in the exponential model and do not restrict the model flexibility too much.

```{r, echo=FALSE, out.width="60%", out.height="30%", fig.cap="Piror distributions", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/gamma_priors.png'))
``` 

For other three biomarkers, we are fitting a Gaussian model. It is important to note that we are fitting similar model to each of the protein LYVE1, REG1B and TFF1 but they are not connected to each other. 

We need prior distributions for the mean value and for the standard deviation. The prior of the mean is chosen so that it is sensible but not restricting. In Part 2, we have seen that the values of all three proteins after logarithmic transformation, obtain both negative and positive values. This is why we will choose the mean of the prior of the parameter $\mu$ as 0. To be able to easily adjust to the data we will chose large value for the standard deviation as 20. Hence the prior distribution for the mean is $\mathcal{N}(0,20)$. In sensitivity analysis, we will notice that the choice of the standard deviation does not affect the result strongly as long as it is wide enough. The prior distribution for $\sigma$ is chosen as $\Gamma(1,1)$ so that it stays positive and gives wide enough range of values.

For the hierarchical model, we need to define prior distributions for the hyperparameters. Here we use same logic as above and choose prior distributions for all the hyperparameters that are required to be positive as $\Gamma(1,1)$ and hyperparameter of the mean of the normal distribution as $\mathcal{N}(0,20)$. These priors provide enough flexibility to fit models to the different diagnosis groups. 

# 5 Stan code

## Pooled model

``` {eval="FALSE"}
// ------------------------------------------
// This is a pooled Stan model for the
// "Urinary biomarkers for pancreatic cancer"
// data set.  
// ------------------------------------------

data {
  // Lengths of the observation vectors
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  // Observations of the 4 protein levels for each group
  vector[4] y1[N1];
  vector[4] y2[N2];
  vector[4] y3[N3];
}

transformed data {
  vector[3] log_y1[N1];
  vector[3] log_y2[N2];
  vector[3] log_y3[N3];
  
  for (j in 1:3){
    log_y1[,j] = log(y1[,j+1]);
    log_y2[,j] = log(y2[,j+1]);
    log_y3[,j] = log(y3[,j+1]);
  }
}

parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  vector [3] mu;
  vector<lower=0> [3] sigma;
}

model {
  // Priors
  alpha ~ gamma(1, 1);
  beta ~ gamma(0.5, 1);  
  for (j in 1:3) {
    mu[j] ~ normal(0,20);
    sigma[j] ~ gamma(1, 1);
  }
  // Likelihoods
  y1[,1] ~ gamma(alpha, beta);
  y2[,1] ~ gamma(alpha, beta);
  y3[,1] ~ gamma(alpha, beta);
  for (j in 1:3) {
    log_y1[,j] ~ normal(mu[j], sigma[j]);
    log_y2[,j] ~ normal(mu[j], sigma[j]);
    log_y3[,j] ~ normal(mu[j], sigma[j]);
  }
}

generated quantities {
  // Posterior predictive distributions
  vector[4] ypred_1;
  vector[4] ypred_2;
  vector[4] ypred_3;
  // Log-likelihoods of the posterior draws
  vector[4] log_lik_1[N1];
  vector[4] log_lik_2[N2];
  vector[4] log_lik_3[N3];
  
  // Group 1
  ypred_1[1] = gamma_rng(alpha, beta);
  for (n in 1:N1)
    log_lik_1[n,1] = gamma_lpdf(y1[n,1] | alpha, beta);
    
  for (j in 1:3) {
    ypred_1[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N1) {
      log_lik_1[n,j+1] = normal_lpdf(log_y1[n,j] | mu[j], sigma[j]);
    }
  }
  //Group 2
  ypred_2[1] = gamma_rng(alpha, beta);
  for (n in 1:N2)
    log_lik_2[n,1] = gamma_lpdf(y2[n,1] | alpha, beta);
    
  for (j in 1:3) {
    ypred_2[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N2) {
      log_lik_2[n,j+1] = normal_lpdf(log_y2[n,j] | mu[j], sigma[j]);
    }
  }
  //Group 3
  ypred_3[1] = gamma_rng(alpha, beta);
  for (n in 1:N3)
      log_lik_3[n,1] = gamma_lpdf(y3[n,1] | alpha, beta);
      
  for (j in 1:3) {
    ypred_3[j+1] = normal_rng(mu[j], sigma[j]);
    for (n in 1:N3) {
      log_lik_3[n,j+1] = normal_lpdf(log_y3[n,j] | mu[j], sigma[j]);
    }
  }
}

```

## Hierarchical model

``` {eval="FALSE"}
// ------------------------------------------
// This is a hierarchical Stan model for the
// "Urinary biomarkers for pancreatic cancer"
// dataset.  
// ------------------------------------------

data {
  // Lengths of the observation vectors
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  // Observations for each group
  vector[4] y1[N1];
  vector[4] y2[N2];
  vector[4] y3[N3];
}

transformed data {
  vector[3] log_y1[N1];
  vector[3] log_y2[N2];
  vector[3] log_y3[N3];
  
  for (j in 1:3){
    log_y1[,j] = log(y1[,j+1]);
    log_y2[,j] = log(y2[,j+1]);
    log_y3[,j] = log(y3[,j+1]);
  }
}

parameters {
  // Hyperparamters
  real<lower=0> alphaP[2];
  real<lower=0> betaP[2];
  vector[3] muP[2];
  vector<lower=0>[3] sigmaP[2];
  // Parameters
  real<lower=0> alpha[3];
  real<lower=0> beta[3];
  vector [3] mu[3];
  vector<lower=0> [3] sigma[3];
}

model {
  // k-> number of parameters in priors
  // j-> number of proteins
  // Hyperpriors
  for (k in 1:2){
    alphaP[k] ~ gamma(1,1);
    betaP[k] ~ gamma(1,1);
    sigmaP[k,] ~ gamma(1,1);
  }
  muP[1,] ~ normal(0,20);
  muP[2,] ~ gamma(1,1);
  // Priors
  alpha ~ gamma(alphaP[1], betaP[1]);
  beta ~ gamma(alphaP[2], betaP[2]);
  for (j in 1:3){
    mu[,j] ~ normal(muP[1,j], sigmaP[1,j]);
    sigma[,j] ~ gamma(muP[2,j], sigmaP[2,j]);
  }
  // Likelihood
  y1[,1] ~ gamma(alpha[1], beta[1]);
  y2[,1] ~ gamma(alpha[2], beta[2]);
  y3[,1] ~ gamma(alpha[3], beta[3]);
  for (j in 1:3){
    log_y1[,j] ~ normal(mu[1,j], sigma[1,j]);
    log_y2[,j] ~ normal(mu[2,j], sigma[2,j]);
    log_y3[,j] ~ normal(mu[3,j], sigma[3,j]);
  }
}

generated quantities {
  // Posterior predictive distributions
  vector[4] ypred_1;
  vector[4] ypred_2;
  vector[4] ypred_3;
  // Log-likelihoods of the posterior draws
  vector[4] log_lik_1[N1];
  vector[4] log_lik_2[N2];
  vector[4] log_lik_3[N3];
  
  // Group 1
  ypred_1[1] = gamma_rng(alpha[1], beta[1]);
  for (n in 1:N1)
    log_lik_1[n,1] = gamma_lpdf(y1[n,1] | alpha[1], beta[1]);
    
  for (j in 1:3) {
    ypred_1[j+1] = normal_rng(mu[1,j], sigma[1,j]);
    for (n in 1:N1) {
      log_lik_1[n,j+1] = normal_lpdf(log_y1[n,j] | mu[1,j], sigma[1,j]);
    }
  }
  //Group 2
  ypred_2[1] = gamma_rng(alpha[2], beta[2]);
  for (n in 1:N2)
    log_lik_2[n,1] = gamma_lpdf(y2[n,1] | alpha[2], beta[2]);
    
  for (j in 1:3) {
    ypred_2[j+1] = normal_rng(mu[2,j], sigma[2,j]);
    for (n in 1:N2) {
      log_lik_2[n,j+1] = normal_lpdf(log_y2[n,j] | mu[2,j], sigma[2,j]);
    }
  }
  //Group 3
  ypred_3[1] = gamma_rng(alpha[3], beta[3]);
  for (n in 1:N3)
      log_lik_3[n,1] = gamma_lpdf(y3[n,1] | alpha[3], beta[3]);
      
  for (j in 1:3) {
    ypred_3[j+1] = normal_rng(mu[3,j], sigma[3,j]);
    for (n in 1:N3) {
      log_lik_3[n,j+1] = normal_lpdf(log_y3[n,j] | mu[3,j], sigma[3,j]);
    }
  }
}

```

# 6 Fitting and convergence diagnostics

Both of the models were first run with the default options of the `rstan::stan()` function, that is

- Number of chains: 4
- Chain length: 2000
- Warmup length: floor(chain length / 2)

Below is part of the output (due to generation of log-likelihoods, the whole output has over 2000 rows) of `monitor(pooled_fit)`:

```{r, echo=FALSE}
pooled_res <- read.csv('./diagnostic_data/pooled_res.csv')
print(pooled_res[c(1:8),c(1:5)])
```

As we can see from the output, the pooled model seems to converge excellently with the default options. There are zero divergences, all $\hat{R}$ values differ from 1 by no more than roughly $10^{-3}$, and the effective sample sizes are above 3000. Hence, there was no need to tweak the options in this case. 

The fitting of the hierarchical model with default options raises a warning of a bit over 200 divergent post-warmup transitions Our first attempt to get that number down was to double the chain length, which seemed to have at least some positive effect. Then we lowered the sampling step size by setting the `adapt_delta` parameter to 0.99. This had a very good effect, and we were able to get the number of divergences to under 20. At this point, we decided to accept the result. Below is an example code block, showing how the hierarchical model was run.

```{r, eval=FALSE}
hier_fit <- stan(
  file = 'path/to/repository/cancer-biomarker-analysis/models/hierarchical.stan',
  data = stan_data,
  iter = 4000,
  control = list(adapt_delta = 0.99)
)
```

A similar glance to the monitor output, as with the pooled model, yields the following:

```{r, echo=FALSE}
hier_res <- read.csv('./diagnostic_data/hier_res.csv')
print(hier_res[c(17:40),c(1:5)])
```

Judging by the effective sample sizes and $\hat{R}$ values, the hierarchical model seems to converge even better than the pooled one (after the corrections to the sampling options). However, we need to keep in mind the divergence warning we were not able to get rid of completely. All of the model parameter estimations show neat convergence, so we can conclude that some of the log-likelihood estimations diverge.

# 7 Posterior predictive checks

Posterior predictive checks were completed visually in this project. The stan models include generation of predictive samples for each diagnosis group and protein. In general the posterior predictive check was done as follows. For both models, the same number of samples as in the original dataset was drawn four times. This samples were plotted as histograms so that the distribution can be compared to the original histogram. Below you can find the figures. The original data is plotted as white histogram on the background and colored histograms present the draws.

For the pooled model, we are using same parameters for each diagnosis group. Therefore, it is relevant to look at the data of each protein as one entity. 

We can see that for creatinine and REG1B replicated samples correspond to the original data quite well. For REG1B, the normal distribution cannot present the longer tail on the left that the actual data has. However, the overall fit is satisfying. 

LYVE1 and TFF1 cause more reason to worry since the original data, after log-transformation, has two peaks meaning that the actual distribution would be bimodal. The normal distribution does not replicate well compared to the data because it is rather a compromise between these two peaks. However, pooled model is already a combination of all of the diagnosis groups so it could be expected that the fit to the data are not as good.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for the pooled model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_pooled_creatinine.png','./images/post_check_pooled_LYVE1.png', './images/post_check_pooled_REG1B.png', './images/post_check_pooled_TFF1.png'))
``` 

For hierarchical model, we obtain overall of twelve figures that are presented below grouped by the diagnosis. Similarly as in the pooled model, replicated datasets for creatinine and REG1B correspond well to the original data for each diagnosis group. The same issue for REG1B can be seen here as the skewness of the data cannot be taken into account with normal distribution. This is a clear improvement point but especially for diagnosis group 3 the fit is actually quite good and covers all the extreme values.

For LYVE2 and TFF1 the same issue with two peaks in the data can be seen, especially with diagnosis group 1 and 2. It is interesting to see that in diagnosis group 1 with healthy people, there is two clear peaks in the data. For diagnosis group 2, the skewness of bimodel distribution makes the normal distribution very bad choice because the fitted distribution needs to be very wide. For protein LYVE1 and diagnosis group 3, the fit is actually quite good even though there are some extreme values on the left that normal distribution cannot replicate. 

For TFF1, the issues are very similar but the other peak is not as large so the issues are not as strong. In general, the replicate data is better but there is still definitely room for improvement. It might be a good idea to fit bimodal distribution instead of normal distribution and this will be discussed further on Part 11.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for diagnosis 2 and the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_hier_creatinine_1.png','./images/post_check_hier_LYVE1_1.png', './images/post_check_hier_REG1B_1.png', './images/post_check_hier_TFF1_1.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for diagnosis 2 and the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_hier_creatinine_2.png','./images/post_check_hier_LYVE1_2.png', './images/post_check_hier_REG1B_2.png', './images/post_check_hier_TFF1_2.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior predictive checks for diagnosis 3 and the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_check_hier_creatinine_3.png','./images/post_check_hier_LYVE1_3.png', './images/post_check_hier_REG1B_3.png', './images/post_check_hier_TFF1_3.png'))
``` 

# 8 Model comparison

In the Stan models, we generate the log-likelihoods for the posterior draws, which are required in order to perform Leave one out -cross validation with Pareto-smoothed importance sampling (PSIS-LOO) to compare the two models (pooled and hierarchical). After extracting the log-likelihoods from the Stan fit, we proceed to perform PSIS-LOO using the `loo` package, store the results in the `diagnostics` matrix, and plot the pareto-k values. In the case of the hierarchical model, the procedure looks like this:

```{r, eval=FALSE}
# Initialize the diagnostics matrix
diagnostics <- matrix(0, nrow = 3, ncol = 2,
                      dimnames = list(
                        c('Group 1', 'Group 2', 'Group 3'),
                        c('ELPD', 'P_eff')
                      )
)

# Compute diagnostics for each group
for (i in 1:3) {
  log_lik <- log_liks[[i]]
  r_eff <- relative_eff(exp(log_lik)) # Relative efficiency
  loo <- loo(log_lik, r_eff=r_eff) # LOO object
  
  estimates <- loo$estimates
  elpd <- estimates[1,1] # PSIS-LOO value
  p_eff <- estimates[2,1] # Effective number of parameters
  
  diagnostics[i,1] = elpd
  diagnostics[i,2] = p_eff
  print(loo, plot_k = TRUE)
}
```

Below are the PSIS-LOO ELPD values and effective numbers of parameters for the pooled and hierarchical model, respectively.

```{r, echo=FALSE}
pooled_eval <- read.csv('./diagnostic_data/pooled_eval.csv')
hier_eval <- read.csv('./diagnostic_data/hier_eval.csv')
print(pooled_eval)
print(hier_eval)
```

To evaluate how reliable our PSIS-LOO evaluations are, we need to take a look at all the respective pareto-k values.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Pareto-k values for each test subject group in the pooled model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/pooled1_pareto_k.png','./images/pooled2_pareto_k.png', './images/pooled3_pareto_k.png'))
```

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Pareto-k values for each test subject group in the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/hier1_pareto_k.png','./images/hier2_pareto_k.png', './images/hier3_pareto_k.png'))
``` 

The k-values for the pooled model are excellent, there are no values above 0.5. Therefore, the ELPD-estimate in this case can be considered reliable. In the hierarchical case, the k-values seem to be in order as well, except for a single bad value in both group 1 and 3. This may be due to the divergences mentioned in chapter 6, since we concluded that the problems lie with the posterior draw log-likelihoods, which are utilized by PSIS-LOO. However, we concluded that a single bad pareto-k value is an uncertainty we can accept. It just needs to be taken into account, when using the results of our model comparison.

Assuming the PSIS-LOO estimates to be reliable, we can thus conclude that the hierarchical model should be preferred over the pooled one, since its ELPD-estimates are considerably higher.

# 9 Predictive performance assessment

This model is not applicable for predictive assessment since it does not use predictors but rather describes the distributions and their differences. Only predictive aspect of the model might be predicting the protein levels of a person based on their diagnosis. But since we are not using any other predictors at this point, it is not relevant or interesting. The results of this model could be used to create a predictive model where the biomarkers are used to predict occurrence of the cancer. These results could guide on how the predictive model could be build and what kind of dependence there are between the cancer and the biomarkers.

# 10 Sensitivity analysis

Sensitivity analysis was carried out by comparing six different scenarios of priors for both models. In both cases, the different resulting posterior distributions were plotted and values of stan monitor checked. Sensitivity analysis was restricted to varying the parameters of the chosen prior distributions.

For the pooled model, the used scenarios are presented below in a table. In the first three scenarios, parameters values are varied only a little. The last three scenarios are extreme cases were the values have been changed drastically. 

```{r echo = FALSE, results = 'asis'}
library(knitr)
S <- matrix(c(1,1,0.5,1,0,20,1,1,
             2,2,1,2,0,10,2,2,
             0.5,0.5,0.1,0.5,0,2,0.5,0.5,
             0.01,0.01,0.01,0.01,10,1,0.01,0.01,
             100,100,100,100,1000,1000,100,100,
             100,0.01,100,0.01,-1000,1,100,0.01), ncol = 8, byrow = T)
df <- data.frame(S)
colnames(df) <- c('Shape of alpha', 'Scale of alpha','Shape of beta', 'Scale of beta',
                  'Mean of mu^j', 'Deviation of mu^j','Shape of sigma^j', 'Scale of sigma^j')
kable(df, caption='Scenarios for sensitivity analysis for pooled model',format.args = list(scientific = FALSE))
```

By looking at the figures we can see that only the scenarios with very large variation in the prior parameters differ seemingly from the others. For all the protein, only scenarios 5 and 6 are separate from other results. For creatinine, the changes are visible but not great. For other three proteins, only scenario 6 really fails to create similar posterior distribution. The reason is that the prior of the mean value of the normal distribution is very narrow and we can safely say that $\mathcal{N}(-1000,1)$ is very unrealistic option. Overall, we can conclude that the model is very robust and not sensitive when varying the parameters of the prior distributions. In addition, the pooled model was fitted smoothly with all of the prior choices which was not the case with the hierarchical model as can be seen below.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Sensitivity analysis for the pooled model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/SA_pooled_creatinine.png','./images/SA_pooled_LYVE1.png', './images/SA_pooled_REG1B.png', './images/SA_pooled_TFF1.png'))
``` 

The same way, we created six scenarios for hierarchical model. Scenarios are listed below in the table. The posterior distributions have been plotted so that each scenario has its one color and each diagnosis group has different line type. Diagnosis 1 is plotted as dashed line, diagnosis 2 as double dashed line and diagnosis 3 as solid line.

```{r echo = FALSE, results = 'asis'}
library(knitr)
S <- matrix(c(1,1,1,1,1,1,0,20,1,1,
              2,2,2,2,2,2,0,10,2,2,
              0.5,0.5,0.5,0.5,0.5,0.5,0,2,0.5,0.5,
              0.01,0.01,0.01,0.01,0.01,0.01,10,1,0.01,0.01,
              100,100,100,100,100,100,1000,1000,100,100,
              100,0.01,100,0.01,100,0.01,-1000,1,100,0.01), ncol = 10, byrow = T)
df <- data.frame(S)
colnames(df) <- c('Shape of alpha_Pk', 'Scale of alpha_Pk','Shape of beta_Pk', 'Scale of beta_Pk',
                  'Mean of mu_P1^j', 'Deviation of mu_P1^j','Shape of mu_P2^j', 'Scale of mu_P2^j',
                  'Shape of sigma_Pk^j', 'Scale of sigma_Pk^j')
kable(df, caption='Scenarios for sensitivity analysis for pooled model',format.args = list(scientific = FALSE))
```

Similarly as in the pooled model, the posterior distribution of creatinine stays quite the same for all the priors but scenarios 5 and 6 are somewhat separate from the others for each diagnosis group. The same kind of behavior can be seen for other proteins as well and even the scenario 6 can be fitted better than in the pooled model. We can conclude that the hierarchical model is not sensitive to the choice of prior parameters since only the unrealistically large changes can be seen in the resulting posterior distributions.

On the other hand, it is noteworthy to say that even though the results end up to be rather good with strange prior distributions, the fitting of the model does not go as smoothly. There occurs a lot of problems initializing the Metropolis algorithm in scenarios 4, 5 and 6. This slows done the computations and might affect other diagnostics of the model. Therefore it is better to use more realistic priors.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Sensitivity analysis for the hierarchical model", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/SA_hier_creatinine.png','./images/SA_hier_LYVE1.png', './images/SA_hier_REG1B.png', './images/SA_hier_TFF1.png'))
``` 

# 11 Discussion

As discussed in Part 7 when making posterior predictive checks, there is multiple possibilities to improve the model. For creatinine, the model works quite well but for the three other biomarkers the Gaussian model could be changed to something more flexible. For example, for REG1B normal distribution fits quite well but skewed distribution could take into account extreme values better. Data could be transformed to positive scale so that, for example, log-normal distribution could be used.    

Secondly, we can see that log-transformed LYVE1 and TFF1 data has two peaks which would imply that the data is bimodally distributed. This is an interesting question since two peaks can be seen within healthy individuals. The reasons for this could be investigated further so that we can conclude whether the explanation is lack of data or that there actually can be observed two kind of individuals. 

Depending on the conclusion, one might consider fitting bimodal distribution or mixture of several distributions. On the other hand, if there is attributes that predict the values of LYVE1 and TFF1, these could be used to create a model that corresponds to the data better. For example, after investigating the data there seems to be dependence between creatinine levels and LYVE1 and TFF1. Trying to fit bimodal distribution to the current data might work but it could result in overfitting since the number of observations might not be large enough.

# 12 Conclusion

To be able to evaluate the results of the data analysis, the resulting posterior distribution for each biomarker has been plotted below. Results from the pooled and the hierarchical model are presented side by side. We can see that creatinine has only small differences between different diagnosis groups. Therefore it can not be used as an indicator for PDAC. This has been the case in the original study [1] and in this project this can be used as a reference.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of creatinine data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and read diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_creatinine.png','./images/post_dist_hier_creatinine.png'))
``` 

For other three biomarkers, the results between pooled and hierarchical model differ significantly and we can conclude that there is differences between the diagnosis groups, at least based on this dataset. Note that green color corresponds to the healthy control group, blue color to the group with some pancreatic disease and red color to the patient with PDAC diagnosis.

When examining the figures fir LYVE1, we can see that diagnosis groups 1 and 3 have very wide distribution but they are somewhat separated. Diagnosis group 2 has quite narrow distribution but it overlaps entirely with group 3. This implies that LYVE1 can be used as an indicator for pancreatic disease but conclusions about PDAC are hard to make.

For REG1B, distributions for each diagnosis group are very similar by shape and for diagnosis group 1 the distribution is very wide. Once again, the distributions of groups 2 and 3 overlap but now largest values of REG1B indicate PDAC. REG1B then could be used as an indicator or predictor for PDAC but differentiating from other pancreatic disease might be difficult.

For TFF1, the distribution for control group is the narrowest so this biomarker is best for recognizing healthy people. The overlap between groups 2 and 3 is small so large values of TFF1 imply some kind of pancreatic disease. The same phenomenon can be seen with TFF1 that it is difficult to separate PDAC from other disease and the distribution of group3 is very wide.

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of LYVE1 data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and read diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_LYVE1.png','./images/post_dist_hier_LYVE1.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of REG1B data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and read diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_REG1B.png','./images/post_dist_hier_REG1B.png'))
``` 

```{r, echo=FALSE, out.width="40%", out.height="20%", fig.cap="Posterior distribution of TFF1 data. The results from the pooled model on the left and from the hierarchical model on the right. Green presents diagnosis 1, blue diagnosis 2 and read diagnosis 3.", fig.show='hold', fig.align='center'}
knitr::include_graphics(c('./images/post_dist_pooled_TFF1.png','./images/post_dist_hier_TFF1.png'))
``` 

# 13 Self-reflection

- Log-muutoksen tarpeellisuus ja kÃ¤ytÃ¤nnÃ¶n toteutus
- Parempi kÃ¤sitys mallien valinnasta (pooled/hierarchical, exponential/bayesian) -> KotitehtÃ¤vissÃ¤ aina annettu valmiiksi mitÃ¤ kÃ¤yttÃ¤Ã¤, mutta nyt piti itse pohtia.
- Convergence ja miten parantaa
- Posterior predictive check kÃ¤ytÃ¤nnÃ¶ssÃ¤, ei tehty vielÃ¤ missÃ¤Ã¤n tehtÃ¤vÃ¤ssÃ¤

# References

-   [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7031151/
-   [2] Debernardi S, O'Brien H, Algahmdi AS, Malats N, Stewart GD, PljeÅ¡a-Ercegovac M, et al. (2020) *A combination of urinary biomarker panel and PancRISK score for earlier detection of pancreatic cancer: A case--control study.* PLoS Med 17(12): e1003489. https://doi.org/10.1371/journal.pmed.1003489
- [3] Bayesian Data Analysis, Third edition; Gelman, Carlin, Stern, Dunson, Vehtari and Rubin; 2016
